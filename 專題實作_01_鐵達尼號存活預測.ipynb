{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+nGULKWPDpePK3jrXb2Zc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jentzufred/titanic_analysis/blob/main/%E5%B0%88%E9%A1%8C%E5%AF%A6%E4%BD%9C_01_%E9%90%B5%E9%81%94%E5%B0%BC%E8%99%9F%E5%AD%98%E6%B4%BB%E9%A0%90%E6%B8%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "專題實作 #01: 鐵達尼號存活預測\n"
      ],
      "metadata": {
        "id": "ehzlRIiGFJYe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xA_zDWowE6vd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a52a997-ae5e-4794-eb41-910c04b219d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.06980851528714314\n",
            "-0.06491041993052589\n",
            "-0.07722109457217768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]Linear Regression Training Accuracy: 0.33831374108814427\n",
            "[0]Logistic Regression Training Accuracy: 0.7783783783783784\n",
            "[0]K Neighbors Training Accuracy: 0.7675675675675676\n",
            "[0]SVC linear  Training Accuracy: 0.7783783783783784\n",
            "[0]SVC RBF Training Accuracy: 0.6756756756756757\n",
            "[0]Gaussian NB Training Accuracy: 0.772972972972973\n",
            "[0]Decision Tree Training Accuracy: 1.0\n",
            "[0]Random Forest Training Accuracy: 0.9783783783783784\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/dsindy/kaggle-titanic/master/data/train.csv')\n",
        "\n",
        "#transform category variable to numerical variable\n",
        "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
        "df['Embarked'] = LabelEncoder().fit_transform(df['Embarked'])\n",
        "\n",
        "#fill null with different ways which are mean, nedian, mode\n",
        "df_fill_mean = df['Age'].fillna(df['Age'].mean())\n",
        "df_fill_median = df['Age'].fillna(df['Age'].median())\n",
        "df_fill_mode = df['Age'].fillna(df['Age'].mode())\n",
        "#let's test which way woulb be better\n",
        "corr_df_mean = df['Survived'].corr(df_fill_mean)\n",
        "corr_df_median = df['Survived'].corr(df_fill_median)\n",
        "corr_df_mode = df['Survived'].corr(df_fill_mode)\n",
        "print(corr_df_mean)\n",
        "print(corr_df_median)\n",
        "print(corr_df_mode)\n",
        "#可以看的出來用眾數做為填充值相對於其他方法對於存活的相關係數較高\n",
        "\n",
        "#feature selcetion\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "#create new features\n",
        "df['Title'] = df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
        "df[\"FamilySize\"] = df['SibSp'] + df['Parch'] +1\n",
        "df['TicketPrefix'] = df['Ticket'].apply(lambda x: x.split()[0] if len(x.split()) > 1 else 'None' )\n",
        "df[['Title', 'FamilySize', 'TicketPrefix']].head()\n",
        "df = df.drop(columns=['PassengerId', 'Cabin', 'Ticket', 'Name'])\n",
        "df[\"Title\"] = LabelEncoder().fit_transform(df['Title'])\n",
        "df['TicketPrefix'] = LabelEncoder().fit_transform(df['TicketPrefix'])\n",
        "\n",
        "columns_X = df.iloc[:, [1,2,3,4,5,6,7,8,9]]\n",
        "columns_y = df['Survived']\n",
        "\n",
        "\n",
        "train_X = columns_X\n",
        "train_y = columns_y\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def models(train_X, train_y):\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    log = LogisticRegression(random_state = 0)\n",
        "    log.fit(train_X, train_y)\n",
        "\n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "    knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
        "    knn.fit(train_X, train_y)\n",
        "\n",
        "    from sklearn.svm import SVC\n",
        "    svc_rbf = SVC(kernel = 'rbf', random_state= 0)\n",
        "    svc_rbf.fit(train_X, train_y)\n",
        "    svc_lin = SVC(kernel = 'linear', random_state= 0 )\n",
        "    svc_lin.fit(train_X, train_y)\n",
        "\n",
        "    from sklearn.naive_bayes import GaussianNB\n",
        "    gauss = GaussianNB()\n",
        "    gauss.fit(train_X, train_y)\n",
        "\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    tree = DecisionTreeClassifier(criterion= 'entropy', random_state= 0 )\n",
        "    tree.fit(train_X, train_y)\n",
        "\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    forest = RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0 )\n",
        "    forest.fit(train_X, train_y)\n",
        "\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    linear = LinearRegression()\n",
        "    linear.fit(train_X, train_y)\n",
        "\n",
        "    print('[0]Linear Regression Training Accuracy:', linear.score(train_X, train_y))\n",
        "    print('[0]Logistic Regression Training Accuracy:', log.score(train_X, train_y))\n",
        "    print('[0]K Neighbors Training Accuracy:', knn.score(train_X, train_y))\n",
        "    print('[0]SVC linear  Training Accuracy:', svc_lin.score(train_X, train_y))\n",
        "    print('[0]SVC RBF Training Accuracy:', svc_rbf.score(train_X, train_y))\n",
        "    print('[0]Gaussian NB Training Accuracy:', gauss.score(train_X, train_y))\n",
        "    print('[0]Decision Tree Training Accuracy:', tree.score(train_X, train_y))\n",
        "    print('[0]Random Forest Training Accuracy:', forest.score(train_X, train_y))\n",
        "\n",
        "    return log, knn, svc_lin, svc_rbf, gauss, tree, forest\n",
        "\n",
        "model = models(train_X, train_y)"
      ]
    }
  ]
}